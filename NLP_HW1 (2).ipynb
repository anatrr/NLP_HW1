{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW1"
      ],
      "metadata": {
        "id": "ALBXSfXQ-0hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install nltk spacy beautifulsoup4 kaggle\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download he_core_news_sm\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer,SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImBmahLF72ZA",
        "outputId": "18bd67e8-49fd-40ef-bb7d-3d95b8819fdd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "\u001b[38;5;1m✘ No compatible package found for 'he_core_news_sm' (spaCy v3.7.5)\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize nltk components\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load spaCy models\n",
        "nlp_spacy = spacy.load('en_core_web_sm')\n",
        "# nlp_spacy_he = spacy.load('he_core_news_sm')"
      ],
      "metadata": {
        "id": "ftks8uHA8ElG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52371c77-2fcb-4395-b43a-c6f065b15657"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading & Basic Analysis"
      ],
      "metadata": {
        "id": "wvnFOS0j8Vl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_data = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
        "spam_data.columns = ['label', 'text', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
        "spam_data = spam_data[['label', 'text']]"
      ],
      "metadata": {
        "id": "TDl8IL4f8kr2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print basic statistics on the data\n",
        "total_messages = len(spam_data)\n",
        "spam_messages = len(spam_data[spam_data['label'] == 'spam'])\n",
        "ham_messages = len(spam_data[spam_data['label'] == 'ham'])\n",
        "\n",
        "word_counts = spam_data['text'].apply(lambda x: len(word_tokenize(x)))\n",
        "avg_words_per_message = word_counts.mean()\n",
        "\n",
        "all_words = nltk.FreqDist(word.lower() for message in spam_data['text'] for word in word_tokenize(message))\n",
        "#most_frequent_words = all_words.most_common(5)\n",
        "words_only_once = sum(1 for count in all_words.values() if count == 1)\n",
        "\n",
        "\n",
        "def average_words_per_message(df):\n",
        "    \"\"\"Calculate and print the average number of words per message.\"\"\"\n",
        "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
        "    average_words = df['word_count'].mean()\n",
        "    print(f\"Average number of words per message: {average_words:.2f}\")\n",
        "    return average_words\n",
        "\n",
        "\n",
        "def most_frequent_words_and_total_word_count(words, n=5):\n",
        "    \"\"\"Calculate and print the n most frequent words.\"\"\"\n",
        "    # Count the total number of words\n",
        "    total_words = len(words)\n",
        "    # Print the total number of words\n",
        "    print(f\"Total number of words: {total_words}\")\n",
        "    # Count the frequency of each word in the list\n",
        "    word_counts = Counter(words)\n",
        "    # Get the n most common words\n",
        "    most_common_words = word_counts.most_common(n)\n",
        "     # Print the top n most frequent words\n",
        "    print(f\"Top {n} most frequent words:\")\n",
        "    for word, count in most_common_words:\n",
        "         print(f\"{word}: {count}\")\n",
        "    return most_common_words\n",
        "\n",
        "def words_appearing_once(df):\n",
        "    \"\"\"Calculate and print the number of words that appear only once.\"\"\"\n",
        "    all_words = ' '.join(df['text']).split()\n",
        "    word_counts = Counter(all_words)\n",
        "    words_once = [word for word, count in word_counts.items() if count == 1]\n",
        "    num_words_once = len(words_once)\n",
        "    print(f\"Number of words that appear only once: {num_words_once}\")\n",
        "    return num_words_once\n",
        "\n",
        "all_words = ' '.join(spam_data['text']).split()\n",
        "most_common_words = most_frequent_words_and_total_word_count(all_words)\n",
        "average_words = average_words_per_message(spam_data)\n",
        "words_only_once = words_appearing_once(spam_data)\n",
        "\n",
        "print(f\"Total number of SMS messages: {total_messages}\")\n",
        "print(f\"Number of spam messages: {spam_messages}\")\n",
        "print(f\"Number of ham messages: {ham_messages}\")\n",
        "print(f\"Number of words that only appear once: {words_only_once}\")\n"
      ],
      "metadata": {
        "id": "zPCUjebs84eT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee751422-6abc-4ce8-8e4c-eb9dd4b22fad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words: 86335\n",
            "Top 5 most frequent words:\n",
            "to: 2134\n",
            "you: 1622\n",
            "I: 1466\n",
            "a: 1327\n",
            "the: 1197\n",
            "Average number of words per message: 15.49\n",
            "Number of words that appear only once: 9268\n",
            "Total number of SMS messages: 5572\n",
            "Number of spam messages: 747\n",
            "Number of ham messages: 4825\n",
            "Number of words that only appear once: 9268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Processing"
      ],
      "metadata": {
        "id": "3WvBo7Uw8_pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import nltk\n",
        "from spacy.lang.en import English\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "# Download NLTK punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize spaCy English tokenizer outside the function\n",
        "nlp = English()\n",
        "tokenizer_spacy = Tokenizer(nlp.vocab)\n",
        "\n",
        "def tokenize_nltk(text, verbose=False):\n",
        "    # Measure the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Tokenize the text using NLTK\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Measure the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    if verbose:\n",
        "        # Print the execution time\n",
        "        print(\"NLTK tokenize time:\", end_time - start_time, \"seconds\")\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def tokenize_spacy(text, verbose=False):\n",
        "    # Measure the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Tokenize the text using spaCy\n",
        "    tokens = [token.text for token in tokenizer_spacy(text)]\n",
        "\n",
        "    # Measure the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    if verbose:\n",
        "        # Print the execution time\n",
        "        print(\"spaCy tokenize time:\", end_time - start_time, \"seconds\")\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example usage:\n",
        "text = spam_data['text'].str.cat(sep=' ')\n",
        "tokens_nltk = tokenize_nltk(text, verbose=True)\n",
        "print(tokens_nltk[:200])\n",
        "most_common_words = most_frequent_words_and_total_word_count(tokens_nltk)\n",
        "\n",
        "print(\"--------------------------\")\n",
        "tokens_spacy = tokenize_spacy(text, verbose=True)\n",
        "print(tokens_spacy[:200])\n",
        "most_common_words = most_frequent_words_and_total_word_count(tokens_spacy)\n",
        "\n",
        "# Apply tokenization to DataFrame without verbose output\n",
        "spam_data['tokens_nltk'] = spam_data['text'].apply(lambda x: tokenize_nltk(x, verbose=False))\n",
        "spam_data['tokens_spacy'] = spam_data['text'].apply(lambda x: tokenize_spacy(x, verbose=False))\n"
      ],
      "metadata": {
        "id": "80DW3Dj-9M-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59aea592-33d9-41a8-d506-b325d3f6ac55"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK tokenize time: 1.1792049407958984 seconds\n",
            "['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...', 'Ok', 'lar', '...', 'Joking', 'wif', 'u', 'oni', '...', 'Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 'T', '&', 'C', \"'s\", 'apply', '08452810075over18', \"'s\", 'U', 'dun', 'say', 'so', 'early', 'hor', '...', 'U', 'c', 'already', 'then', 'say', '...', 'Nah', 'I', 'do', \"n't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here', 'though', 'FreeMsg', 'Hey', 'there', 'darling', 'it', \"'s\", 'been', '3', 'week', \"'s\", 'now', 'and', 'no', 'word', 'back', '!', 'I', \"'d\", 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', '?', 'Tb', 'ok', '!', 'XxX', 'std', 'chgs', 'to', 'send', ',', 'å£1.50', 'to', 'rcv', 'Even', 'my', 'brother', 'is', 'not', 'like', 'to', 'speak', 'with', 'me', '.', 'They', 'treat', 'me', 'like', 'aids', 'patent', '.', 'As', 'per', 'your', 'request', \"'Melle\", 'Melle', '(', 'Oru', 'Minnaminunginte', 'Nurungu', 'Vettam', ')', \"'\", 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'Callers', '.', 'Press', '*', '9', 'to', 'copy', 'your', 'friends', 'Callertune', 'WINNER', '!', '!', 'As', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', 'å£900']\n",
            "Total number of words: 104164\n",
            "Top 5 most frequent words:\n",
            ".: 4857\n",
            "to: 2148\n",
            "I: 1956\n",
            "you: 1888\n",
            ",: 1871\n",
            "--------------------------\n",
            "spaCy tokenize time: 0.5317368507385254 seconds\n",
            "['Go', 'until', 'jurong', 'point,', 'crazy..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'Cine', 'there', 'got', 'amore', 'wat...', 'Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...', 'Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', \"rate)T&C's\", 'apply', \"08452810075over18's\", 'U', 'dun', 'say', 'so', 'early', 'hor...', 'U', 'c', 'already', 'then', 'say...', 'Nah', 'I', \"don't\", 'think', 'he', 'goes', 'to', 'usf,', 'he', 'lives', 'around', 'here', 'though', 'FreeMsg', 'Hey', 'there', 'darling', \"it's\", 'been', '3', \"week's\", 'now', 'and', 'no', 'word', 'back!', \"I'd\", 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still?', 'Tb', 'ok!', 'XxX', 'std', 'chgs', 'to', 'send,', 'å£1.50', 'to', 'rcv', 'Even', 'my', 'brother', 'is', 'not', 'like', 'to', 'speak', 'with', 'me.', 'They', 'treat', 'me', 'like', 'aids', 'patent.', 'As', 'per', 'your', 'request', \"'Melle\", 'Melle', '(Oru', 'Minnaminunginte', 'Nurungu', \"Vettam)'\", 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'Callers.', 'Press', '*9', 'to', 'copy', 'your', 'friends', 'Callertune', 'WINNER!!', 'As', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', 'å£900', 'prize', 'reward!', 'To', 'claim', 'call', '09061701461.', 'Claim', 'code', 'KL341.', 'Valid', '12', 'hours', 'only.', 'Had', 'your', 'mobile', '11', 'months', 'or', 'more?', 'U', 'R', 'entitled', 'to', 'Update', 'to', 'the', 'latest', 'colour', 'mobiles', 'with', 'camera', 'for', 'Free!', 'Call']\n",
            "Total number of words: 86960\n",
            "Top 5 most frequent words:\n",
            "to: 2134\n",
            "you: 1622\n",
            "I: 1466\n",
            "a: 1327\n",
            "the: 1197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that spaCy is significantly more efficient and less time-consuming than NLTK. One of the reasons for this efficiency is the optimized nature of spaCy's tokenization rules, which are designed for speed. In terms of time complexity, both methods operate at O(n).\n",
        "Additionally, the output from NLTK and spaCy differs slightly."
      ],
      "metadata": {
        "id": "Dv4oicYeemJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the SMS text using nltk and spaCy\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_nltk(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def lemmatize_spacy(text):\n",
        "    return [token.lemma_ for token in nlp_spacy(text)]\n",
        "\n",
        "spam_data['lemmas_nltk'] = spam_data['tokens_nltk'].apply(lemmatize_nltk)\n",
        "spam_data['lemmas_spacy'] = spam_data['text'].apply(lemmatize_spacy)"
      ],
      "metadata": {
        "id": "y-26iDMC9jvS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stem the SMS text using nltk and spaCy\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_nltk(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "def stem_spacy(text):\n",
        "    return [token.lemma_ for token in nlp_spacy(text)]  # Using lemma as spaCy doesn't have a built-in stemmer\n",
        "\n",
        "spam_data['stems_nltk'] = spam_data['tokens_nltk'].apply(stem_nltk)\n",
        "spam_data['stems_spacy'] = spam_data['text'].apply(stem_spacy)\n"
      ],
      "metadata": {
        "id": "jlLcTsNL9pSn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison of nltk and spaCy implementations\n",
        "\n",
        "def updated_statistics(tokens):\n",
        "    all_words = nltk.FreqDist(word.lower() for word_list in tokens for word in word_list)\n",
        "    most_frequent_words = all_words.most_common(5)\n",
        "    words_only_once = sum(1 for count in all_words.values() if count == 1)\n",
        "    return most_frequent_words, words_only_once\n",
        "\n",
        "print(\"Updated statistics after tokenization (nltk):\", updated_statistics(spam_data['tokens_nltk']))\n",
        "print(\"Updated statistics after lemmatization (nltk):\", updated_statistics(spam_data['lemmas_nltk']))\n",
        "print(\"Updated statistics after stemming (nltk):\", updated_statistics(spam_data['stems_nltk']))\n",
        "print(\"Updated statistics after tokenization (spaCy):\", updated_statistics(spam_data['tokens_spacy']))\n",
        "print(\"Updated statistics after lemmatization (spaCy):\", updated_statistics(spam_data['lemmas_spacy']))\n",
        "print(\"Updated statistics after stemming (spaCy):\", updated_statistics(spam_data['stems_spacy']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mocOKoJu9vZB",
        "outputId": "1c7e531e-6c08-4d79-d709-4163b68e9958"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated statistics after tokenization (nltk): ([('.', 4886), ('i', 2900), ('to', 2241), ('you', 2228), (',', 1871)], 4992)\n",
            "Updated statistics after lemmatization (nltk): ([('.', 4886), ('i', 2900), ('to', 2241), ('you', 2228), (',', 1871)], 4779)\n",
            "Updated statistics after stemming (nltk): ([('.', 4886), ('i', 2900), ('to', 2241), ('you', 2228), (',', 1871)], 4179)\n",
            "Updated statistics after tokenization (spaCy): ([('to', 2226), ('i', 2208), ('you', 1917), ('a', 1419), ('the', 1317)], 7989)\n",
            "Updated statistics after lemmatization (spaCy): ([('.', 4945), ('i', 3741), ('be', 3260), ('to', 2309), ('you', 2217)], 4583)\n",
            "Updated statistics after stemming (spaCy): ([('.', 4945), ('i', 3741), ('be', 3260), ('to', 2309), ('you', 2217)], 4583)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scraping"
      ],
      "metadata": {
        "id": "GZpdRhtq95Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use BeautifulSoup to scrape text data from a public page on one of your social media profiles.\n",
        "url = 'https://en.wikipedia.org/wiki/English_Springer_Spaniel'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "print(soup.title)\n",
        "\n",
        "scraped_text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
        "print(\"Scraped Text:\", scraped_text[:500])  # Print the first 500 characters to verify\n",
        "\n"
      ],
      "metadata": {
        "id": "1Nl-XqaC97mS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529c4819-de17-44ef-8d2e-3aff3940490e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<title>English Springer Spaniel - Wikipedia</title>\n",
            "Scraped Text: \n",
            " The English Springer Spaniel is a breed of gun dog in the Spaniel group traditionally used for flushing and retrieving game. They are descended from the Norfolk or Shropshire Spaniels of the mid-19th century; the breed has diverged into separate show and working lines. It is closely related to the Welsh Springer Spaniel and very closely to the English Cocker Spaniel; less than a century ago, springers and cockers would come from the same litter. The smaller \"cockers\" were used in woodcock hunt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform tokenization, lemmatization, and stemming on the scraped text.\n",
        "scraped_tokens_nltk = tokenize_nltk(scraped_text)\n",
        "scraped_tokens_spacy = tokenize_spacy(scraped_text)\n",
        "scraped_lemmas_nltk = lemmatize_nltk(scraped_tokens_nltk)\n",
        "scraped_lemmas_spacy = lemmatize_spacy(scraped_text)\n",
        "scraped_stems_nltk = stem_nltk(scraped_tokens_nltk)\n",
        "scraped_stems_spacy = stem_spacy(scraped_text)"
      ],
      "metadata": {
        "id": "bPUx6u_W-Acy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print word statistics on the scraped data before and after text processing.\n",
        "print(\"Scraped text statistics before processing:\", updated_statistics([scraped_tokens_nltk]))\n",
        "print(\"Scraped text statistics after tokenization (nltk):\", updated_statistics([scraped_tokens_nltk]))\n",
        "print(\"Scraped text statistics after lemmatization (nltk):\", updated_statistics([scraped_lemmas_nltk]))\n",
        "print(\"Scraped text statistics after stemming (nltk):\", updated_statistics([scraped_stems_nltk]))\n",
        "print(\"Scraped text statistics after tokenization (spaCy):\", updated_statistics([scraped_tokens_spacy]))\n",
        "print(\"Scraped text statistics after lemmatization (spaCy):\", updated_statistics([scraped_lemmas_spacy]))\n",
        "print(\"Scraped text statistics after stemming (spaCy):\", updated_statistics([scraped_stems_spacy]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14HyTPjO-EqN",
        "outputId": "78bd9364-3524-41cb-9e97-72a2d571f476"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped text statistics before processing: ([('the', 102), ('.', 67), (',', 63), ('and', 45), ('to', 44)], 390)\n",
            "Scraped text statistics after tokenization (nltk): ([('the', 102), ('.', 67), (',', 63), ('and', 45), ('to', 44)], 390)\n",
            "Scraped text statistics after lemmatization (nltk): ([('the', 102), ('.', 67), (',', 63), ('and', 45), ('to', 44)], 368)\n",
            "Scraped text statistics after stemming (nltk): ([('the', 102), ('.', 67), (',', 63), ('and', 45), ('to', 44)], 331)\n",
            "Scraped text statistics after tokenization (spaCy): ([('the', 102), ('and', 45), ('to', 44), ('a', 33), ('of', 33)], 438)\n",
            "Scraped text statistics after lemmatization (spaCy): ([('the', 103), ('be', 62), (',', 56), ('and', 45), ('to', 44)], 358)\n",
            "Scraped text statistics after stemming (spaCy): ([('the', 103), ('be', 62), (',', 56), ('and', 45), ('to', 44)], 358)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WhatsApp Analysis"
      ],
      "metadata": {
        "id": "AI6Pcq89-I5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import a .txt file of at least 50 WhatsApp messages in Hebrew.\n",
        "with open('whatsapp.txt', 'r', encoding='utf-8') as file:\n",
        "    whatsapp_text = file.readlines()\n",
        "    all_text = ' '.join(whatsapp_text)\n",
        "print(all_text[:300])"
      ],
      "metadata": {
        "id": "zfjTk1pI-QtL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a00e361-66cb-4050-e990-1bcb5fa8c333"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/01/2024 13:10] +972 54-567-2517: השיעור התחיל ?כי הזום לא פעיל\n",
            " [02/01/2024 13:11] לירן מדמח: לא\n",
            " [02/01/2024 13:11] +972 54-567-2517: תודה\n",
            " [02/01/2024 14:00] לירן מדמח: תתנו לעמית להיות מנהל בבקשה\n",
            " [02/01/2024 14:00] +972 54-426-2231: כן תתנו לי\n",
            " [02/01/2024 14:05] לירן מדמח: למה אתה לא פה\n",
            " [0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Tokenize, lemmatize, and stem the WhatsApp data.\n",
        "\n",
        "def display_word_statistics(tokens, title):\n",
        "    total_words = len(tokens)\n",
        "    word_counts = Counter(tokens)\n",
        "    most_common_words = word_counts.most_common(5)\n",
        "    print(f\"{title} - Total number of words: {total_words}\")\n",
        "    print(f\"{title} - Top 5 most frequent words:\")\n",
        "    for word, count in most_common_words:\n",
        "        print(f\"{word}: {count}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def tokenize_hebrew(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    hebrew_tokens = [token for token in tokens if token.isalnum() and any('\\u0590' <= char <= '\\u05EA' for char in token)]\n",
        "    return hebrew_tokens\n",
        "\n",
        "# דוגמה לשימוש:\n",
        "hebrew_tokens = tokenize_hebrew(all_text)\n",
        "print(hebrew_tokens[:100])\n",
        "display_word_statistics(hebrew_tokens, \"hebrew_tokens\")\n"
      ],
      "metadata": {
        "id": "69Y_Q2tU-WNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3ef8b5-26af-4729-b7e5-6d8cc4649287"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['השיעור', 'התחיל', 'כי', 'הזום', 'לא', 'פעיל', 'לירן', 'מדמח', 'לא', 'תודה', 'לירן', 'מדמח', 'תתנו', 'לעמית', 'להיות', 'מנהל', 'בבקשה', 'כן', 'תתנו', 'לי', 'לירן', 'מדמח', 'למה', 'אתה', 'לא', 'פה', 'לירן', 'מדמח', 'בואו', 'הוא', 'מתחיל', 'לירן', 'מדמח', 'לירן', 'מדמח', 'התחלנו', 'לא', 'רואים', 'אתכם', 'בזום', 'לירן', 'מדמח', 'חוזרים', 'ביקשת', 'לתזכר', 'אותך', 'כאן', 'להעלות', 'למודל', 'סמינר', 'דוגמא', 'מסמך', 'בסיס', 'לסמינר', 'תודה', 'לירן', 'מדמח', 'חזרנו', 'ללמוד', 'מה', 'שלומכם', 'נשאלתי', 'לגבי', 'תוצרים', 'של', 'הקורס', 'להרחיב', 'את', 'הדעת', 'ב', 'ללמוד', 'לקרוא', 'לשחזר', 'את', 'הקוד', 'שמוצג', 'במאמר', 'לערוך', 'דיון', 'בתוצאות', 'השוואה', 'לריצות', 'שמאמר', 'השוואה', 'לאלגוריתמים', 'בין', 'אלגוריתמים', 'וכו', 'איפה', 'אנחנו', 'היום', 'סטס', 'שליח', 'פיליפ', 'פיקוס', 'לא', 'לשכוח', 'מאנשי', 'לירן', 'מדמח']\n",
            "hebrew_tokens - Total number of words: 815\n",
            "hebrew_tokens - Top 5 most frequent words:\n",
            "לירן: 17\n",
            "מדמח: 16\n",
            "לא: 13\n",
            "של: 13\n",
            "את: 12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_hebrew(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    hebrew_tokens = [token for token in tokens if token.isalnum() and any('\\u0590' <= char <= '\\u05EA' for char in token)]\n",
        "\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in hebrew_tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "\n",
        "lemmatized_tokens = lemmatize_hebrew(all_text)\n",
        "\n",
        "print(lemmatized_tokens[:100])\n",
        "display_word_statistics(lemmatized_tokens, \"lemmatized_tokens\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9zwXQ8K-cg2",
        "outputId": "addb81c5-d56c-4c63-a9fd-3502defc207b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['השיעור', 'התחיל', 'כי', 'הזום', 'לא', 'פעיל', 'לירן', 'מדמח', 'לא', 'תודה', 'לירן', 'מדמח', 'תתנו', 'לעמית', 'להיות', 'מנהל', 'בבקשה', 'כן', 'תתנו', 'לי', 'לירן', 'מדמח', 'למה', 'אתה', 'לא', 'פה', 'לירן', 'מדמח', 'בואו', 'הוא', 'מתחיל', 'לירן', 'מדמח', 'לירן', 'מדמח', 'התחלנו', 'לא', 'רואים', 'אתכם', 'בזום', 'לירן', 'מדמח', 'חוזרים', 'ביקשת', 'לתזכר', 'אותך', 'כאן', 'להעלות', 'למודל', 'סמינר', 'דוגמא', 'מסמך', 'בסיס', 'לסמינר', 'תודה', 'לירן', 'מדמח', 'חזרנו', 'ללמוד', 'מה', 'שלומכם', 'נשאלתי', 'לגבי', 'תוצרים', 'של', 'הקורס', 'להרחיב', 'את', 'הדעת', 'ב', 'ללמוד', 'לקרוא', 'לשחזר', 'את', 'הקוד', 'שמוצג', 'במאמר', 'לערוך', 'דיון', 'בתוצאות', 'השוואה', 'לריצות', 'שמאמר', 'השוואה', 'לאלגוריתמים', 'בין', 'אלגוריתמים', 'וכו', 'איפה', 'אנחנו', 'היום', 'סטס', 'שליח', 'פיליפ', 'פיקוס', 'לא', 'לשכוח', 'מאנשי', 'לירן', 'מדמח']\n",
            "lemmatized_tokens - Total number of words: 815\n",
            "lemmatized_tokens - Top 5 most frequent words:\n",
            "לירן: 17\n",
            "מדמח: 16\n",
            "לא: 13\n",
            "של: 13\n",
            "את: 12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_hebrew(text):\n",
        "    stemmer = SnowballStemmer(\"porter\")\n",
        "    tokens = word_tokenize(text)\n",
        "    hebrew_tokens = [token for token in tokens if token.isalnum() and any('\\u0590' <= char <= '\\u05EA' for char in token)]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in hebrew_tokens]\n",
        "    return stemmed_tokens\n",
        "stemmed_tokens = stem_hebrew(all_text)\n",
        "\n",
        "print(stemmed_tokens[:100])\n",
        "display_word_statistics(stemmed_tokens, \"stemmed_tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68AJgQKKJ_b",
        "outputId": "8f82f966-20c6-4eb8-916a-20752e4fa944"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['השיעור', 'התחיל', 'כי', 'הזום', 'לא', 'פעיל', 'לירן', 'מדמח', 'לא', 'תודה', 'לירן', 'מדמח', 'תתנו', 'לעמית', 'להיות', 'מנהל', 'בבקשה', 'כן', 'תתנו', 'לי', 'לירן', 'מדמח', 'למה', 'אתה', 'לא', 'פה', 'לירן', 'מדמח', 'בואו', 'הוא', 'מתחיל', 'לירן', 'מדמח', 'לירן', 'מדמח', 'התחלנו', 'לא', 'רואים', 'אתכם', 'בזום', 'לירן', 'מדמח', 'חוזרים', 'ביקשת', 'לתזכר', 'אותך', 'כאן', 'להעלות', 'למודל', 'סמינר', 'דוגמא', 'מסמך', 'בסיס', 'לסמינר', 'תודה', 'לירן', 'מדמח', 'חזרנו', 'ללמוד', 'מה', 'שלומכם', 'נשאלתי', 'לגבי', 'תוצרים', 'של', 'הקורס', 'להרחיב', 'את', 'הדעת', 'ב', 'ללמוד', 'לקרוא', 'לשחזר', 'את', 'הקוד', 'שמוצג', 'במאמר', 'לערוך', 'דיון', 'בתוצאות', 'השוואה', 'לריצות', 'שמאמר', 'השוואה', 'לאלגוריתמים', 'בין', 'אלגוריתמים', 'וכו', 'איפה', 'אנחנו', 'היום', 'סטס', 'שליח', 'פיליפ', 'פיקוס', 'לא', 'לשכוח', 'מאנשי', 'לירן', 'מדמח']\n",
            "stemmed_tokens - Total number of words: 815\n",
            "stemmed_tokens - Top 5 most frequent words:\n",
            "לירן: 17\n",
            "מדמח: 16\n",
            "לא: 13\n",
            "של: 13\n",
            "את: 12\n",
            "\n"
          ]
        }
      ]
    }
  ]
}